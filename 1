******************************************         scala     **********************************************



111111111111*****************************************      Understanding Conditional and looping statements in Scala     ***********************************111111111111111111


#######     if-else condition   ################

object project {

  def main(args: Array[String]): Unit = {
    // val name: String = "HELLO WELCOME TO SCALA SESSION"
    var num: Int = 29

    // println(name)
    // println(num)

    if (num % 2 == 0) {
      println("Even number " + num)
    } else {
      println("Odd number " + num)
    }
  }
}


#######     while do while   ################


object project {

  def main(args: Array[String]): Unit = {
    var num: Int = 8

    println("While loop condition")
    while (num <= 15) {
      println("Number=" + num)
      num += 1
    }

    println("do-While loop condition")
    var num1: Int = 8

    do {
      println("Num=" + num1)
      num1 += 1
    } while (num1 < 9)
  }
}


#######    for loop  ################

object project {

  def main(args: Array[String]): Unit = {
    println("For loop using until keyword")
    for (x <- 0 until 5) {
      println("num=" + x)
    }

    println("For loop using to keyword")
    for (a <- 1 to 3; b <- 0 to 2) {
      println("a=" + a)
      println("b=" + b)
    }
  }
}


#######    defining a function   ################

object functions {

  // define function using return statement
  def add(x: Int, y: Int): Int = {
    return x + y
  }

  // define function without using return statement
  def sub(x: Int, y: Int): Int = {
    x - y
  }

  // define inline function
  def mult(x: Int, y: Int): Int = return x * y

  // def inline without using return statement
  def divide(x: Int, y: Int): Int = x / y

  // def inline without using return type
  def divide1(x: Int, y: Int) = x / y

  // define main function
  def main(args: Array[String]): Unit = {
    println("Addition..." + add(10, 5))
    println("Subtraction..." + sub(10, 5))
    println("Multiply...." + mult(10, 5))
    println("Division..." + divide(10, 5))
    println("Division..." + divide1(10, 2))
  }
}



#######     anonymous function   ################


// Anonymous Function
var add = (x: Int, y: Int) => x + y
println("Addition..." + add(10, 5))



#######     operator as function   ################


// Operator as a function:
object func {

  def main(args: Array[String]): Unit = {
    println(func2.*(5, 4))
    println(func3.+(5, 4))
  }
}

object func2 {

  def *(x: Int, y: Int): Unit = {
    println("Multiply...." + (x * y)) // without using return type
  } // here the output will be () because it do not contain return statement
}

object func3 {

  def +(x: Int, y: Int): Int = {
    println("Addition...")
    return (x + y) // using return statement
  }
}



#########   Currying Function   ############

object func {

  def main(args: Array[String]): Unit = {
    // Calling currying function
    println(add(10)(10))

    val sum = add(30) // Calling Partial function work as wildcard Character
    println(sum(25))

    println("............")
    // Calling another currying function
    println(add3(20)(10))

    val sum20 = add3(10) _ // Calling Parcial function work as wildcard Character
    println(sum20(50))
  }

  // Currying function
  def add(x: Int) = (y: Int) => x + y

  // Another signature for function currying where partial apply can work
  def add3(x: Int)(y: Int) = x + y // Parcial function
}



222222222222222*****************************************     Understanding basic data types in Scala: List, Set, Tuples & Map     ***********************************222222222222222222



#######################   Set     #############################


object list1 {
 def main(args:Array[String]): Unit ={
 println(set1)
 println(set2)
 println(set1++set2) //concatenate
 println(set2+"Hiii...") //adding element in the set
 println(set1.head)
 println(set2.tail)
 println(set2.isEmpty)
 set2.foreach(println) //using for loop
 for(sets<-set3 ){
 println(sets)
      }
  }
 var set1:Set[Int]=Set(1,2,3,4,5,6,7,8)
 var set2:Set[String]=Set("Hello","Scala","Welcome")
 var set3=Set(1,2,3,4,"Hello")
 }



########################    MAP    ###################



object Main {




 val mymap: Map[Int, String] = Map(801 -> "Max", 802 -> "Tom", 803 -> "Jack")




 def main(args: Array[String]): Unit = {


   println(mymap)




   mymap.keys.foreach { key =>


     println("key " + key)


     println("Val " + mymap(key))


   }




   println(mymap.contains(801))


   println(mymap.keySet)


   println(mymap.values)


 }


}


#######################   Dictionary    #############################

object dict{
 def main(args:Array[String]): Unit ={
 println(dict1)
 println(dict2)
 println(dict1.keys) //printing all the keys
 println(dict2.contains(104)) //checking the value is present or 
not
 println(dict1.values) //printing all the values
 println(dict1++dict2) //concatenate
 dict1.foreach(println)
 //for printing keys and value
 dict1.keys.foreach{key=>
 println("Keys="+key)
 println("Values="+dict1(key))
    }
  }
 var dict1:Map[Int,String]=Map(101->"Hello",102->"Scala",103
->"Welcome",102->"Data")
 var dict2:Map[Int,String]=Map(104->"ETDS",105->"ML",106->"Scala")
 }



#######################   Tuples   #############################

object tup{
 def main(args:Array[String]):Unit={
 println(t)
 println(t1)
 t1.productIterator.foreach(println)//using loop
 println(t1._3)//printing the ele of given position
 println(t._5)//printing the ele of given position
 t.productIterator.foreach{
      i=>println(i)
    }
 println(t2)
 println(t2._4._1)//printing ele from nested tuple
 }
 var t=(1,2,3,"Hello","Scala")
 var t1=Tuple4(1,2,34,5)
 val t2=Tuple8(2,3,4,("Helo","World"),"Hi",1,3,4)
 }


#######################   List     #############################


 object list2{
 def main(args:Array[String]):Unit={
 println("List of type int="+a)
 println("List of type String="+b)
 println("Hii"::b)// append at first Element
 println("List of type dynamic="+c)
 for(i<-c){
 println("="+i)
    }
 a.foreach(println)//using for each loop
 println("Empty list="+List(Nil))
 println("Reverse="+a.reverse)
 println("Creating list of values="+List.fill(2,3)(4,2,1))
 println("Index="+b(2))
 var sum:Int=0
 a.foreach(sum+=_)
 println("Sum of list="+sum)
 println("Delimiter="+a.mkString(".")) //mkstring is use for giving 
delimiter
 }
 var a:List[Int]=List(12,3,45,6)
 var b:List[String]=List("Hello","Scala","Welcome")
 val c=List(1,2,3,4,5,"Hello","Scala")
 }





333333333333333333**************************     Implementing class and constructor concepts       ********************************3333333333333333333333

#######################    Normal class and Object     #############################


class demo {
 var no:Int=100;
  var nameofcompany:String="Apple";
  def show(): Unit ={
 println("Name of the company= "+nameofcompany)
 println("Total number of product="+no)
  }
 }
 object main{
 def main(args:Array[String]): Unit ={
 var d1=new demo()
    d1.show()
  }
 }



#######################     Anonymous object   #############################


class demo2{
 var no: Int = 200;
  var nameofcompany: String = "Samsung";
  def show(): Unit = {
 println("Name of the company= " + nameofcompany)
 println("Total number of product=" + no)
  }
 }
 object main2{
 def main(args:Array[String]): Unit ={
 new demo2().show() //Anonymous object
 }
 }


#######################      Constructor   #############################


 class demo3(var a:Int,var b:Int){
  def add(): Unit ={
    var c=a+b;
    println("Addition="+c)
  }
  def multi(): Unit ={
    var d=a*b
    println("Multiplication="+d)
  }
 }
 object main3{
  def main(args:Array[String]): Unit ={
    var ad=new demo3(10,20)
    var ad1=new demo3(10,10)
    ad.add()
    ad1.add()
    ad.multi()
    ad1.multi()
  }
 }


#######################                  Constructor  Overloading                  #############################


class demo3(var a:Int,var b:Int){
  var s:Int=30
  def add(): Unit ={
    var c=a+b;
    println("Addition="+c)
  }
  def sub(): Unit ={
    var d=a-s
    println("Subtraction="+d)
  }
  //Auxiliary constructor i.e Overloading
  def this(a:Int,b:Int,s:Int){
    this(a,b)
    this.s=s
  }
 }
 object main3{
  def main(args:Array[String]): Unit ={
    var ad=new demo3(10,20,10)
    var ad1=new demo3(10,10)
    ad.add()
    ad1.add()
    ad.sub()
    ad1.sub()
  }
 }





4444444444444444444****************************************            Implementing the type of Inheritance                        ************************************4444444444444444444

#######################      Single Level Inheritance #############################

 //Single Inheritance with method override
 class stud{
  var name:String="Sam"
  val age:Int=20
  def disp(): Unit = {
    println("Hello Scala")
  }
 }
 class stud2 extends stud{
  def show(): Unit = {
    println("Name of student="+name)
  }
  override def disp(): Unit = {
    println("Age=" + age)
  }
 }
 object student{
  def main(args:Array[String]): Unit = {
    var s=new stud2()
    var s1=new stud()
    s.show()
    s1.disp()
    s.disp()
  }
 }



#######################               Multi Level Inheritance          #############################


//Multi-level inheritance
 class number{
  var no1:Int=10
 }
 class number1  extends number {
  var no2:Int=20
 }
 class number2 extends number1{
  var add=no1 + no2
  def show(): Unit = {
    println("Adition="+add)
  }
 }
 object addition{
  def main(args:Array[String]): Unit = {
    var a=new number2()
Page | 2
    a.show()
  }
 }


#######################            Multiple Inheritance: (2 parent with 1 )child                  ############################


//trait keyword is use for Multiple inheritance
 //trait is used to extend multiple parent claas with one single child 
class
 trait Printable {
  def print() //Abstract method
 }
 trait Showable{
  //def show()
  var b:Int=10
 }
 class A6 extends Printable with Showable{
  def print(): Unit ={
    println("This is printable....")
  }
  //non-Abstract method
  def show(): Unit ={
    println("This is showable...."+b)
  }
 }
 object scala{
  def main(args:Array[String]): Unit ={
    var a=new A6()
    a.print()
    a.show()
  }
 }


##########      Using abstract class and trait together:
 If you call trait first with extend error will occur   ######################


//Using abstract and trait class together
 trait printable {
  def print() //Abstract method
 }
 abstract class showable{
  //def show()
  var b:Int=10
 }
 //always extend abstract class first than trait otherwise we will get error
 class A1 extends printable with showable {
  def print(): Unit ={
    println("This is printable....")
  }
  //non-Abstract method
  def show(): Unit ={
    println("This is showable...."+b)
  }
 }
 object scala1{
  def main(args:Array[String]): Unit ={
    var a=new A1()
    a.print()
    a.show()
  }
 }



# If you call abstract class first with extend 


//Using abstract and trait class together
 trait printable {
  def print() //Abstract method
 }
 abstract class showable{
  //def show()
  var b:Int=10
 }
 //always extend abstract class first than trait otherwise we will get error
 class A1 extends showable with printable{
  def print(): Unit ={
    println("This is printable....")
  }
  //non-Abstract method
  def show(): Unit ={
    println("This is showable...."+b)
  }
 }
 object scala1{
  def main(args:Array[String]): Unit ={
    var a=new A1()
    a.print()
    a.show()
  }
 }


#########################         Hierarchical Inheritance: (1 parent with 2 child)                    ###################


//Hierarchical inheritance
 class add{
  var a=10
 }
 class add1 extends add {
  var c=5
  var add=a+c
  def show(): Unit ={
    println("Addition="+add)
  }
 }
 class sub1 extends add{
  var d=5
  var sub=a-d
  def disp(): Unit ={
    println("Subtraction="+sub)
  }
 }
 object multiple{
  def main(args:Array[String]): Unit ={
    var a= new add1()
    a.show()
    var s=new sub1()
    s.disp()
  }
 }



# Using trait keyword


//hierarchical inheritance
 trait add{
  var a=10
 }
 class add1 extends add {
  var c=5
  var add=a+c
  def show(): Unit ={
    println("Addition="+add)
  }
 }
 class sub1 extends add{
  var d=10
  var sub=a-d
  def disp(): Unit ={
    println("Subtraction="+sub)
  }
 }
 object multiple{
  def main(args:Array[String]): Unit ={
    var a= new add1()
    a.show()
    var s=new sub1()
    s.disp()
  }
 }



############      Using Final Keywork:
 A variable with final keyword cannot be override    ###########


 class speed {
 final  var a:Int=30
 }
 class bike extends speed{
 override var a=20
 def disp(): Unit = {
 println(a)
  }
 }
 object bike{
 def main(args:Array[String]):Unit={
 var b=new bike()
    b.disp()
  }
 }

                                               OR


 //Using final keyword
 class speed {
 final  var a:Int=30
 }
 class bike extends speed{
 // override var a=20
 def disp(): Unit = {
 println(a)
  }
 }
 object bike{
 def main(args:Array[String]):Unit={
 var b=new bike()
    b.disp()
  }
 }



######################    Hybrid Inheritance:
 Using Multiple and Multi-level Inheritance in hybrid Method     ####################


 //Hybrid Inheritance
 trait number{
 var a:Int=10
 }
 abstract class number1{
 var b:Int=5
 def print(): Unit
 }
 class sum extends number1 with number{
 def print(): Unit = {
 println("Summing the given values....")
 println("a="+a)
 println("b="+b)
  }
 }
 class addition extends sum{
 var add=a+b
 def show(): Unit = {
 println("Addition="+add)
  }
 }
 object arithmetic{
 def main(args:Array[String]): Unit = {
 var add1=new addition()
    add1.print()
    add1.show()
  }
 }



55555555##################        exception Handling    ###################55555555555555555



import java.io.FileReader
 import java.io.FileNotFoundException
 import java.io.IOException
 object demo {
  def main(args:Array[String]): Unit = {
    try {
      val f = new FileReader("input.txt")
    } catch {
      case ex: FileNotFoundException => {
        println("Missing file exception")
      }
    }
    //2nd exception in the same object
    try {
      var n = 45 / 0
    }
    catch {
      case a: ArithmeticException => {
        println("not correct arithmetics applied")
      }
      case b: FileNotFoundException => {
        println("file missing")
      }
      case c: IOException => {
        println("cannot interpret input or output")
      }
    }
    finally {
      println("Program executed")
    }
  }
 }



#Now with proper path we will not get exception

object demo {
  def main(args: Array[String]): Unit = {
    try {
      val f = new FileReader( fileName = "src/demof.txt")
    } catch {
      case ex: FileNotFoundException => {
        println("Missing file exception")
      }
    } finally {
      println("Program executed")
    }
  }
}


//explicitly throwing error can be done using throw new
 class except{
 def validate(age:Int)={
 if(age<18)
 throw new ArithmeticException("Your are no eligible")
 else println("you are eligible")
  }
 }
 object ex1{
 def main (args:Array[String]): Unit ={
 val th=new except()
    th.validate(12)
https://temp-mail.org/en/  }
 }








*************************************              SPARK                        ****************************************************************





- Word Count with PySpark********************************************


#Program 1:Word Count
#This program will demonstrate creating an Rdd from a text dataset , performing transformations to count the occurences of each word,and actions to collect and print the results.
!pip install pyspark
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("PySparkWordCount").setMaster("local")
sc = SparkContext(conf=conf)
data=["hello world","hello spark","hello RDD","hello PySpark"]
rdd=sc.parallelize(data)
word_rdd_Vidit=rdd.flatMap(lambda sentence:sentence.split(" "))
word_counts_rdd_Vidit=word_rdd_Vidit.map(lambda word:(word,1))
word_counts=word_counts_rdd_Vidit.collect()
for word, count in word_counts:
    print(f"{word}:{count}")
sc.stop()



- Temperature Conversion with PySpark********************************************


!pip install pyspark
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("PySparkTemperatureConversion").setMaster("local")
sc = SparkContext(conf=conf)
celcius_temps_Vidit=[0,40,23,56,78,90]
rdd=sc.parallelize(celcius_temps_Vidit)
farhenheit_rdd_Vidit=rdd.map(lambda c:(c,(c*9/5)+32))
farhenheit_temps_Vidit=farhenheit_rdd_Vidit.collect()
for c,f in farhenheit_temps_Vidit:
  print(f"{c}°C = {f} F")


- Spark Analysis on Iris Dataset********************************************

pip install pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
# Initialize Spark session
spark = SparkSession.builder.appName("IrisDatasetExample").getOrCreate()
# Load the iris dataset
iris_data_rutvi = [
    (5.1, 3.5, 1.4, 0.2, "Iris-setosa"),
    (4.9, 3.0, 1.4, 0.2, "Iris-setosa"),
    (4.7, 3.2, 1.3, 0.2, "Iris-setosa"),
    (7.0, 3.2, 4.7, 1.4, "Iris-versicolor"),
    (6.4, 3.2, 4.5, 1.5, "Iris-versicolor"),
    (6.9, 3.1, 4.9, 1.5, "Iris-versicolor"),
    (5.9, 3.0, 5.1, 1.8, "Iris-virginica"),
    (6.8, 3.0, 5.5, 2.1, "Iris-virginica"),
    (6.7, 3.1, 5.6, 2.4, "Iris-virginica")
]

columns = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"]
# Create DataFrame
df_rutvi = spark.createDataFrame(iris_data_rutvi, schema = columns)
# Show initial data
df_rutvi.show()
# Filter for Iris-setosa species
df_filtered_rutvi = df_rutvi.filter(col("species") == "Iris-setosa")
# Show the filtered data
df_filtered_rutvi.show()
# Stop Spark session
spark.stop()







- Spark Analysis on Wine Quality Dataset ********************************************

pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("WineQualityAnalyzer").getOrCreate()

spark
# Load the wine quality dataset (using PySpark's inbuilt sample datasets)
df_rutvi = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/content/winequality-red.csv")
# Show initial data
df_rutvi.show(5)
# Filter for wines with a quality rating of 7 or higher
df_rutvi_filtered = df_rutvi.filter(col("quality") >= 7)
# Show the filtered data
df_rutvi_filtered.show()
# Stop Spark session
spark.stop()




- All Spark SQL Functions with PySpark********************************************




- ------California Housing Data using Spark SQL with PySpark********************************************

pip install pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, max, min
# Initialize Spark Session
spark = SparkSession.builder.appName("California Housing Analysis").getOrCreate()
# Load Built-in Dataset
# (Assuming that the dataset is available locally in the sample_data directory)
df = spark.read.csv("sample_data/california_housing_train.csv", header=True, inferSchema=True)
# Show the first few rows of the dataset
df.show(5)
# Register DataFrame as Temp Table
df.createOrReplaceTempView("california_housing")
# Example Analysis 1: Calculate average house value by median income bracket
result1 = spark.sql("""
    SELECT median_income, AVG(median_house_value) as avg_house_value
    FROM california_housing
    GROUP BY median_income
    ORDER BY median_income
""")
result1.show()
# Example Analysis 2: Find the maximum and minimum house values in each housing block
result2 = spark.sql("""
    SELECT longitude, latitude, MAX(median_house_value) as max_house_value,
           MIN(median_house_value) as min_house_value
    FROM california_housing
    GROUP BY longitude, latitude
    ORDER BY max_house_value DESC
    LIMIT 10
""")
result2.show()
# Example Analysis 3: Determine the average house value for houses older than 50 years
result3 = spark.sql("""
    SELECT AVG(median_house_value) as avg_old_house_value
    FROM california_housing
    WHERE housing_median_age > 50
""")
result3.show()
# Stop the Spark Session
spark.stop()
from pyspark.sql import SparkSession
# Initialize Spark Session
spark = SparkSession.builder.appName("California Housing Analysis Advance").getOrCreate()
# Load the California Housing Dataset
df = spark.read.csv("sample_data/california_housing_train.csv", header=True, inferSchema=True)
# Register DataFrame as Temp Table
df.createOrReplaceTempView("california_housing")
# 1. Average House Value by Proximity to Ocean (Based on Longitude)
result1 = spark.sql("""
    SELECT CASE
               WHEN longitude < -122 THEN 'Near Ocean'
               ELSE 'Far from Ocean'
           END as proximity_to_ocean,
           AVG(median_house_value) as avg_house_value
    FROM california_housing
    GROUP BY proximity_to_ocean
""")
result1.show()
# 2. Average Number of Rooms by Age of Housing
result2 = spark.sql("""
    SELECT housing_median_age,
           AVG(total_rooms) as avg_rooms
    FROM california_housing
    GROUP BY housing_median_age
    ORDER BY housing_median_age
""")
result2.show()
# 3. Top 10 Locations with the Highest Median Income
result3 = spark.sql("""
    SELECT longitude, latitude, median_income
    FROM california_housing
    ORDER BY median_income DESC
    LIMIT 10
""")
result3.show()
# 4. Relationship Between Population and House Value (Correlation Analysis)
result4 = spark.sql("""
    SELECT POPULATION, median_house_value
    FROM california_housing
""")
result4.corr("population", "median_house_value")
# 5. Average House Value by Income Level (Low, Medium, High)
result5 = spark.sql("""
    SELECT CASE
               WHEN median_income < 2.5 THEN 'Low Income'
               WHEN median_income BETWEEN 2.5 AND 4.5 THEN 'Medium Income'
               ELSE 'High Income'
           END as income_level,
           AVG(median_house_value) as avg_house_value
    FROM california_housing
    GROUP BY income_level
""")
result5.show()
# 6. Number of Houses in Different Age Groups
result6 = spark.sql("""
    SELECT CASE
               WHEN housing_median_age < 20 THEN 'New'
               WHEN housing_median_age BETWEEN 20 AND 40 THEN 'Middle-aged'
               ELSE 'Old'
           END as age_group,
           COUNT(*) as house_count
    FROM california_housing
    GROUP BY age_group
    ORDER BY house_count DESC
""")
result6.show()
# 7. Average Number of Bedrooms Per House
result7 = spark.sql("""
    SELECT AVG(total_bedrooms / households) as avg_bedrooms_per_house
    FROM california_housing
""")
result7.show()
# 8. Median House Value by Latitude and Longitude
result8 = spark.sql("""
    SELECT latitude, longitude,
           PERCENTILE_APPROX(median_house_value, 0.5) as median_value
    FROM california_housing
    GROUP BY latitude, longitude
    ORDER BY median_value DESC
    LIMIT 10
""")
result8.show()
# 9. Households with More Than 4 Persons Per Household
result9 = spark.sql("""
    SELECT COUNT(*) as large_households
    FROM california_housing
    WHERE population / households > 4
""")
result9.show()
# 10. Areas with the Highest House Density (Rooms per Area)
result10 = spark.sql("""
    SELECT latitude, longitude, (total_rooms / housing_median_age) as rooms_density
    FROM california_housing
    ORDER BY rooms_density DESC
    LIMIT 10
""")
result10.show()
# Stop the Spark Session
spark.stop()




- Flight Delays Analysis using Spark SQL with PySpark********************************************


pip install pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count
# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Flights Analysis") \
    .getOrCreate()

# Load the flights dataset
flights = spark.read.format("csv") \
.option("header", "true") \
    .option("inferSchema", "true") \
    .load("/content/departuredelays.csv")
# Create a temporary view of the DataFrame
flights.createOrReplaceTempView("flights")
# Perform analysis using Spark SQL
# Example 1: Average delay by origin airport

avg_delay_by_origin = spark.sql("""
    SELECT origin, AVG(delay) as avg_delay
    FROM flights
    GROUP BY origin
    ORDER BY avg_delay DESC
    LIMIT 10
""")
avg_delay_by_origin.show()

# Example 2: Total flights and average delay by day of week

flights_by_day = spark.sql("""
    SELECT
        date,
        COUNT(*) as total_flights,
        AVG(delay) as avg_delay
    FROM flights
    GROUP BY date
    ORDER BY date
    LIMIT 7
""")
flights_by_day.collect()
flights_by_day.show()
# Example 3: Top 5 routes with the highest total delay

top_delayed_routes = spark.sql("""
    SELECT
        origin,
        destination,
        SUM(delay) as total_delay,
        COUNT(*) as flight_count
    FROM flights
    GROUP BY origin, destination
    ORDER BY total_delay DESC
    LIMIT 5
""")
top_delayed_routes.show()
# Show results
print("Top 10 origins by average delay:")
avg_delay_by_origin.show()

print("\nFlights and average delay by day (first week):")
flights_by_day.show()

print("\nTop 5 routes with highest total delay:")
top_delayed_routes.show()

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count, hour, month
# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Extended Flights Analysis") \
    .getOrCreate()

# Load the flights dataset
flights = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/content/departuredelays.csv")
# Create a temporary view of the DataFrame
flights.createOrReplaceTempView("flights")

# Example 4: Average delay by origin airport (top 10)
avg_delay_by_origin = spark.sql("""
    SELECT origin, AVG(delay) as avg_delay
    FROM flights
    GROUP BY origin
    ORDER BY avg_delay DESC
    LIMIT 10
""")
# Example 5: Top 5 busiest routes
busiest_routes = spark.sql("""
    SELECT
        origin,
        destination,
        COUNT(*) as flight_count
    FROM flights
    GROUP BY origin, destination
    ORDER BY flight_count DESC
    LIMIT 5
""")
# Example 6: Monthly flight trends

monthly_trends = spark.sql("""
    SELECT
        SUBSTRING(CAST(date AS STRING), 5, 2) as month,
        COUNT(*) as total_flights,
        AVG(delay) as avg_delay
    FROM flights
    GROUP BY SUBSTRING(CAST(date AS STRING), 5, 2)
    ORDER BY month
""")
# Example 7: Percentage of delayed flights by origin
delayed_percentage = spark.sql("""
    SELECT
        origin,
        COUNT(*) as total_flights,
        SUM(CASE WHEN delay > 0 THEN 1 ELSE 0 END) as delayed_flights,
        (SUM(CASE WHEN delay > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) as delayed_percentage
    FROM flights
    GROUP BY origin
    ORDER BY delayed_percentage DESC
    LIMIT 10
""")
# Example 8: Average delay by hour of day
delay_by_hour = spark.sql("""
    SELECT
        CAST(SUBSTRING(CAST(date AS STRING), 10, 2) AS INT) as hour,
        AVG(delay) as avg_delay
    FROM flights
    GROUP BY SUBSTRING(CAST(date AS STRING), 10, 2)
    ORDER BY hour
""")

# Show results
print("Top 10 origins by average delay:")
avg_delay_by_origin.show()

print("\nTop 5 busiest routes:")
busiest_routes.show()

print("\nMonthly flight trends:")
monthly_trends.show()

print("\nTop 10 origins by percentage of delayed flights:")
delayed_percentage.show()

print("\nAverage delay by hour of day:")
delay_by_hour.show()
# Stop the SparkSession
spark.stop()






- Extracting transforming and selecting features using Pyspark********************************************

pip install pyspark
from pyspark.sql import SparkSession
from pyspark.ml.feature import Imputer,StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler, ChiSqSelector
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
# Create a Spark Session

spark = SparkSession.builder.appName("BostonHousingFeatureEngineeringDone").getOrCreate()
# 1. Load the dataset into PySpark
#Assume we have a csv file with headers

data = spark.read.csv("/content/HousingData.csv", header = True, inferSchema = True)
#Print the schema of data types

data.printSchema()
#Define categorical and numerical columns

categorical_cols = ["CHAS", "RAD"]
numerical_cols = ["CRIM", "ZN", "INDUS", "NOX", "RM", "AGE", "DIS", "TAX", "PTRATIO", "B", "LSTAT"]
#Import DoubleType

from pyspark.sql.types import DoubleType

#2. Handling missing values and convert to appropriate types

for col_name in numerical_cols + ["MEDV"]:
  data = data.withColumn(col_name, col(col_name).cast(DoubleType()))
#Use Imputer to handle NaN values in numerical columns

imputer = Imputer(inputCols = numerical_cols, outputCols = numerical_cols)
imputer.setStrategy("mean")
#3. Handle categorical features
# for categorical columns, replace NaN with a placeholder before indexing

from pyspark.sql.functions import when,col
for cat_col in categorical_cols:
  data = data.withColumn(cat_col, when(col(cat_col).isNull(), "Unknown").otherwise(col(cat_col)))

indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_indexed", handleInvalid="keep") for col in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{col}_indexed", outputCol=f"{col}_encoded") for col in categorical_cols]

#4. Scale numerical features

assembler_num = VectorAssembler(inputCols = numerical_cols, outputCol = "num_features")
scaler = StandardScaler(inputCol = "num_features", outputCol = "scaled_num_features", withStd = True, withMean = True)

#5. combine features

encoded_cols = [f"{col}_encoded" for col in categorical_cols]
assembler = VectorAssembler(inputCols = encoded_cols + ["scaled_num_features"], outputCol = "features")

#6. Perform feature selection

median_medv = data.approxQuantile("MEDV", [0.5], 0.01)[0]
data = data.withColumn("label", (col("MEDV") > median_medv).cast("double"))

selector = ChiSqSelector(numTopFeatures = 10, featuresCol = "features", outputCol = "selected_features", labelCol = "label")

#Create and fit the pipeline

pipeline = Pipeline(stages=[imputer] + indexers + encoders + [assembler_num, scaler, assembler, selector])

try:
  model = pipeline.fit(data)
  result = model.transform(data)

  #show the result
  result.select("selected_features", "label").show(5, truncate = False)

  #Get feature importances
  feature_importances = model.stages[-1].selectedFeatures
  print("selected feature indces: ", feature_importances)

except Exception as e:
  print(f"An error occurred: {str(e)}")

#Stop the Spark session

spark.stop()







MLlib 
- Linear Regression using PySpark********************************************

!pip install pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local").appName("linea_regression_model").getOrCreate()
real_estate = spark.read.option("inferSchema", "true").csv("/content/Real estate.csv", header = True)
real_estate.printSchema()
real_estate.show(2)
real_estate.describe().show()
#VectorAssembler to transform data into feature columns

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols = [
    'X1 transaction date',
    'X2 house age',
    'X3 distance to the nearest MRT station',
    'X4 number of convenience stores',
    'X5 latitude',
    'X6 longitude'], outputCol = "features")
data_set = assembler.transform(real_estate)
data_set.select(["features","Y house price of unit area"]).show(2)
#Split into train and test data

train_data, test_data = data_set.randomSplit([0.7, 0.3])
train_data.show(truncate = False)
test_data.show(truncate = False)

# Train your model (Fit your model wth train data)

from pyspark.ml.regression import LinearRegression

lr = LinearRegression(labelCol = "Y house price of unit area")
lrModel = lr.fit(train_data)

#Perform descriptive analysis with correlation

test_stats = lrModel.evaluate(test_data)

print(f"RMSE: {test_stats.rootMeanSquaredError}")
print(f"R2: {test_stats.r2}")
print(f"MSE: {test_stats.meanSquaredError}")



- Logistic Regression using PySpark********************************************

!pip install pyspark

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ml-diabetes").getOrCreate()

df = spark.read.csv("/content/diabetes.csv", header = True, inferSchema = True)
df.printSchema()

import pandas as pd

pd.DataFrame(df.take(5), columns = df.columns).transpose()

df.show()

df.toPandas()
df.groupby("Outcome").count().toPandas()

#making numerical features

numeric_features = [t[0] for t in df.dtypes if t[1] == "int"]

numeric_features
df.select(numeric_features).describe().toPandas().transpose()

from pandas.plotting import scatter_matrix

numeric_data = df.select(numeric_features).toPandas()

axs = scatter_matrix(numeric_data, figsize = (8,8));

#Rotate axis labels and remove axis ticks

n = len(numeric_data.columns)
for i in range(n):
  v = axs[i, 0]
  v.yaxis.label.set_rotation(0)
  v.yaxis.label.set_ha("right")
  v.set_yticks(())
  h = axs[n-1, i]
  h.xaxis.label.set_rotation(90)
  h.set_xticks(())

from pyspark.sql.functions import isnull, when, count, col

df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()

#dropping unnecessary columns

dataset = df.drop("SkinThickness")
dataset = dataset.drop("Insulin")
dataset_new = dataset.drop("DiabetesPedigreeFunction")
dataset_final = dataset_new.drop("Pregnancies")

dataset_final.show()

#Assemble all the features with VectorAssembler

required_features = ["Glucose", "BloodPressure", "BMI", "Age"]

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols = required_features, outputCol = "features")

transformed_data = assembler.transform(dataset_final)
transformed_data.show()


#Split the data

(training_data, test_data) = transformed_data.randomSplit([0.8, 0.2], seed = 20)

print("Training Dataset Count: " + str(training_data.count()))
print("Test Dataset Count: " + str(test_data.count()))


from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(featuresCol = 'features', labelCol = 'Outcome', maxIter=10)
lrModel = lr.fit(training_data)
lr_predictions = lrModel.transform(test_data)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Outcome', metricName = 'accuracy')
print('Logistic Regression Accuracy:', multi_evaluator.evaluate(lr_predictions))




- Decision Tree Classification using PySpark********************************************

pip install pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('ml-diabetes').getOrCreate()

df = spark.read.csv('/diabetes.csv', header=True, inferSchema=True)
df.printSchema()

import pandas as pd

pd.DataFrame(df.take(5), columns=df.columns).transpose()
df.show()
df.toPandas()

df.groupby('Outcome').count().toPandas()
# Making numerical features
numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']
numeric_features
df.select(numeric_features).describe().toPandas().transpose()
from pandas.plotting import scatter_matrix
numeric_data = df.select(numeric_features).toPandas()

axs = scatter_matrix(numeric_data, figsize = (8,8))

# Rotate axis labels and remove axis ticks
n = len(numeric_data.columns)
for i in range(n):
    v = axs[i, 0]
    v.yaxis.label.set_rotation(0)
    v.yaxis.label.set_ha('right')
    v.set_yticks(())
    h = axs[n-1, i]
    h.xaxis.label.set_rotation(90)
    h.set_xticks(())
from pyspark.sql.functions import isnull, when, count, col

df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()
dataset = df.drop('SkinThickness')
dataset = df.drop('Insulin')
dataset.show()
dataset_new = dataset.drop('DiabetesPedigreeFunction')
dataset_new.show()
dataset_final = dataset_new.drop('Pregnancies')

dataset_final.show()
# Assemble all features with VectoeAssembler
required_features = ['Glucose','BloodPressure','BMI','Age']

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols = required_features, outputCol = 'features')

transformed_data = assembler.transform(dataset_final)
transformed_data.show()
# Split the data

(training_data, test_data) = transformed_data.randomSplit([0.8,0.2], seed =2020)

print("Training Dataset Count: " + str(training_data.count()))
print("Test Dataset Count: " + str(test_data.count()))

from pyspark.ml.classification import DecisionTreeClassifier

dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'Outcome', maxDepth = 3)

dtModel = dt.fit(training_data)

dt_predictions = dtModel.transform(test_data)

dt_predictions.select('Glucose', 'BloodPressure', 'BMI', 'Age', 'Outcome').show(10)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Outcome', metricName = 'accuracy')
print('Decision Tree Accuracy:', multi_evaluator.evaluate(dt_predictions))

Gradient-boosted Tree classifier Model

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.

from pyspark.ml.classification import GBTClassifier
gb = GBTClassifier(labelCol = 'Outcome', featuresCol = 'features')
gbModel = gb.fit(training_data)
gb_predictions = gbModel.transform(test_data)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Outcome', metricName = 'accuracy')
print('Gradient-boosted Trees Accuracy:', multi_evaluator.evaluate(gb_predictions))




- Recommendation System using PySpark********************************************
pip install pyspark
import pandas as pd
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local").appName("Movie_recommendation").config("spark.some.config.option","some-value").getOrCreate()
rating = spark.read.format("csv").option("header","true").option("inferSchema", "true").load(r"/content/ratings_small.csv")
rating = rating.drop('timestamp')

movies = spark.read.format("csv").option("header","true").option("inferSchema", "true").load(r"/content/movies_metadata.csv")
movie_data = rating.join(movies, rating.movieId == movies.id)

Columns = len(movie_data.columns)
Rows = movie_data.count()
print('Number of Columns: {}\nNumber of Rows: {}'.format(Columns, Rows))
movie_data.columns

DATA CLEANING: Replace all the zeros in the abaove mentioned fields (except “Pregnancies”) with NaN.
import numpy as np
from pyspark.sql.functions import when

movie_data = movie_data.withColumn("userId",when(movie_data.userId==0,np.nan).otherwise(movie_data.userId))
movie_data = movie_data.withColumn("id",when(movie_data.id==0,np.nan).otherwise(movie_data.id))
movie_data = movie_data.withColumn("rating",when(movie_data.rating==0,np.nan).otherwise(movie_data.rating))
movie_data = movie_data.withColumn("title",when(movie_data.title==0,np.nan).otherwise(movie_data.title))
from pyspark.sql.types import IntegerType
movie_data = movie_data.withColumn("budget", movie_data["budget"].cast(IntegerType()))

movie_data.show()

Correlations between independent variables using data visualization. Selecting different numeric and string columns, and drawing 1 plot (e.g. bar chart, histogram, boxplot, etc.) for each to summarise it

Recommendation engine

Build the recommendation model using ALS on the training data
(training,test) = movie_data.randomSplit([0.8,0.2])
# Build the recommendation model using ALS on the training data
#Fitting the Alternating Least Squares Model

from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS
from pyspark.sql.types import IntegerType # import IntegerType

# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics
als = ALS(maxIter=5,regParam=0.09,rank=25,userCol="userId",itemCol="id",ratingCol="rating",coldStartStrategy="drop",nonnegative=True)

# Convert 'id' column to IntegerType
training = training.withColumn("id", training["id"].cast(IntegerType()))

model = als.fit(training) # fit the ALS model to the training set

Generating Predictions & Model Evaluation: Evaluating a model is a core part of building an effective machine learning model. In PySpark we will be using RMSE(Root mean squared Error) as our evaluation metric. The RMSE described our error in terms of the rating column.

evaluator = RegressionEvaluator(metricName="rmse",labelCol="rating",predictionCol="prediction")

# Convert 'id' column to IntegerType in the test DataFrame
test = test.withColumn("id", test["id"].cast(IntegerType()))

predictions = model.transform(test)
rmse = evaluator.evaluate(predictions)
print("Root-mean-square error = "+str(rmse))
predictions.show()


Recommending Movies with ALS: The approach here will be simple We will be taking a single userid example 39 as features and pass it to trained ALS Model. The same way we did with the test data!

single_user = test.filter(test['userId']==39).select(['id','userId','title','genres'])
# User had 10 ratings in the test data set
# Realistically this should be some sort of hold out set!
single_user.show(truncate = False)


#Now we will use model.transform() function in order to generate recommended movies along with their predicted features.

recomendations = model.transform(single_user)
recomendations.orderBy('prediction',ascending=False).show(truncate = False )









---- Spark streaming********************************************

pip install pyspark

!pip install -q findspark
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Initialize SparkSession and SparkContext
spark = SparkSession.builder.master("local").appName("WordCount").getOrCreate()
sc = spark.sparkContext

# Initialize StreamingContext with a 10-second batch interval
ssc = StreamingContext(sc, 10)

import os

# Create a directory for the stream files
stream_dir = "/content/input.txt"
if not os.path.exists(stream_dir):
    os.mkdir(stream_dir)

# Define the text file stream
lines = ssc.textFileStream(stream_dir)

lines

# Split each line into words and count them
words = lines.flatMap(lambda line: line.split(" "))
word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

# Print the word counts to the console
word_counts.pprint()
word_counts
# Start the streaming computation
ssc.start()

import time

# Create a series of files to simulate the stream
for i in range(5):
    with open(f"/content/file{i}.txt", "w") as f:
        f.write(f"This is a streaming example file {i}.\nThis example shows streaming word count.\n")
    time.sleep(10)  # Sleep for 10 seconds to simulate real-time data

# Wait for the computation to terminate after 60 seconds
#ssc.awaitTerminationOrTimeout(60)
ssc.stop(stopSparkContext=False, stopGraceFully=True)






