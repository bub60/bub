 Word Count with PySpark********************************************


#Program 1:Word Count
#This program will demonstrate creating an Rdd from a text dataset , performing transformations to count the occurences of each word,and actions to collect and print the results.
!pip install pyspark
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("PySparkWordCount").setMaster("local")
sc = SparkContext(conf=conf)
data=["hello world","hello spark","hello RDD","hello PySpark"]
rdd=sc.parallelize(data)
word_rdd_Vidit=rdd.flatMap(lambda sentence:sentence.split(" "))
word_counts_rdd_Vidit=word_rdd_Vidit.map(lambda word:(word,1))
word_counts=word_counts_rdd_Vidit.collect()
for word, count in word_counts:
    print(f"{word}:{count}")
sc.stop()



- Temperature Conversion with PySpark********************************************


!pip install pyspark
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("PySparkTemperatureConversion").setMaster("local")
sc = SparkContext(conf=conf)
celcius_temps_Vidit=[0,40,23,56,78,90]
rdd=sc.parallelize(celcius_temps_Vidit)
farhenheit_rdd_Vidit=rdd.map(lambda c:(c,(c*9/5)+32))
farhenheit_temps_Vidit=farhenheit_rdd_Vidit.collect()
for c,f in farhenheit_temps_Vidit:
  print(f"{c}°C = {f} F")


- Spark Analysis on Iris Dataset********************************************

pip install pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
# Initialize Spark session
spark = SparkSession.builder.appName("IrisDatasetExample").getOrCreate()
# Load the iris dataset
iris_data_rutvi = [
    (5.1, 3.5, 1.4, 0.2, "Iris-setosa"),
    (4.9, 3.0, 1.4, 0.2, "Iris-setosa"),
    (4.7, 3.2, 1.3, 0.2, "Iris-setosa"),
    (7.0, 3.2, 4.7, 1.4, "Iris-versicolor"),
    (6.4, 3.2, 4.5, 1.5, "Iris-versicolor"),
    (6.9, 3.1, 4.9, 1.5, "Iris-versicolor"),
    (5.9, 3.0, 5.1, 1.8, "Iris-virginica"),
    (6.8, 3.0, 5.5, 2.1, "Iris-virginica"),
    (6.7, 3.1, 5.6, 2.4, "Iris-virginica")
]

columns = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"]
# Create DataFrame
df_rutvi = spark.createDataFrame(iris_data_rutvi, schema = columns)
# Show initial data
df_rutvi.show()
# Filter for Iris-setosa species
df_filtered_rutvi = df_rutvi.filter(col("species") == "Iris-setosa")
# Show the filtered data
df_filtered_rutvi.show()
# Stop Spark session
spark.stop()







- Spark Analysis on Wine Quality Dataset ********************************************

pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("WineQualityAnalyzer").getOrCreate()

spark
# Load the wine quality dataset (using PySpark's inbuilt sample datasets)
df_rutvi = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/content/winequality-red.csv")
# Show initial data
df_rutvi.show(5)
# Filter for wines with a quality rating of 7 or higher
df_rutvi_filtered = df_rutvi.filter(col("quality") >= 7)
# Show the filtered data
df_rutvi_filtered.show()
# Stop Spark session
spark.stop()




- All Spark SQL Functions with PySpark********************************************




- ------California Housing Data using Spark SQL with PySpark********************************************

pip install pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, max, min
# Initialize Spark Session
spark = SparkSession.builder.appName("California Housing Analysis").getOrCreate()
# Load Built-in Dataset
# (Assuming that the dataset is available locally in the sample_data directory)
df = spark.read.csv("sample_data/california_housing_train.csv", header=True, inferSchema=True)
# Show the first few rows of the dataset
df.show(5)
# Register DataFrame as Temp Table
df.createOrReplaceTempView("california_housing")
# Example Analysis 1: Calculate average house value by median income bracket
result1 = spark.sql("""
    SELECT median_income, AVG(median_house_value) as avg_house_value
    FROM california_housing
    GROUP BY median_income
    ORDER BY median_income
""")
result1.show()
# Example Analysis 2: Find the maximum and minimum house values in each housing block
result2 = spark.sql("""
    SELECT longitude, latitude, MAX(median_house_value) as max_house_value,
           MIN(median_house_value) as min_house_value
    FROM california_housing
    GROUP BY longitude, latitude
    ORDER BY max_house_value DESC
    LIMIT 10
""")
result2.show()
# Example Analysis 3: Determine the average house value for houses older than 50 years
result3 = spark.sql("""
    SELECT AVG(median_house_value) as avg_old_house_value
    FROM california_housing
    WHERE housing_median_age > 50
""")
result3.show()
# Stop the Spark Session
spark.stop()
from pyspark.sql import SparkSession
# Initialize Spark Session
spark = SparkSession.builder.appName("California Housing Analysis Advance").getOrCreate()
# Load the California Housing Dataset
df = spark.read.csv("sample_data/california_housing_train.csv", header=True, inferSchema=True)
# Register DataFrame as Temp Table
df.createOrReplaceTempView("california_housing")
# 1. Average House Value by Proximity to Ocean (Based on Longitude)
result1 = spark.sql("""
    SELECT CASE
               WHEN longitude < -122 THEN 'Near Ocean'
               ELSE 'Far from Ocean'
           END as proximity_to_ocean,
           AVG(median_house_value) as avg_house_value
    FROM california_housing
    GROUP BY proximity_to_ocean
""")
result1.show()
# 2. Average Number of Rooms by Age of Housing
result2 = spark.sql("""
    SELECT housing_median_age,
           AVG(total_rooms) as avg_rooms
    FROM california_housing
    GROUP BY housing_median_age
    ORDER BY housing_median_age
""")
result2.show()
# 3. Top 10 Locations with the Highest Median Income
result3 = spark.sql("""
    SELECT longitude, latitude, median_income
    FROM california_housing
    ORDER BY median_income DESC
    LIMIT 10
""")
result3.show()
# 4. Relationship Between Population and House Value (Correlation Analysis)
result4 = spark.sql("""
    SELECT POPULATION, median_house_value
    FROM california_housing
""")
result4.corr("population", "median_house_value")
# 5. Average House Value by Income Level (Low, Medium, High)
result5 = spark.sql("""
    SELECT CASE
               WHEN median_income < 2.5 THEN 'Low Income'
               WHEN median_income BETWEEN 2.5 AND 4.5 THEN 'Medium Income'
               ELSE 'High Income'
           END as income_level,
           AVG(median_house_value) as avg_house_value
    FROM california_housing
    GROUP BY income_level
""")
result5.show()
# 6. Number of Houses in Different Age Groups
result6 = spark.sql("""
    SELECT CASE
               WHEN housing_median_age < 20 THEN 'New'
               WHEN housing_median_age BETWEEN 20 AND 40 THEN 'Middle-aged'
               ELSE 'Old'
           END as age_group,
           COUNT(*) as house_count
    FROM california_housing
    GROUP BY age_group
    ORDER BY house_count DESC
""")
result6.show()
# 7. Average Number of Bedrooms Per House
result7 = spark.sql("""
    SELECT AVG(total_bedrooms / households) as avg_bedrooms_per_house
    FROM california_housing
""")
result7.show()
# 8. Median House Value by Latitude and Longitude
result8 = spark.sql("""
    SELECT latitude, longitude,
           PERCENTILE_APPROX(median_house_value, 0.5) as median_value
    FROM california_housing
    GROUP BY latitude, longitude
    ORDER BY median_value DESC
    LIMIT 10
""")
result8.show()
# 9. Households with More Than 4 Persons Per Household
result9 = spark.sql("""
    SELECT COUNT(*) as large_households
    FROM california_housing
    WHERE population / households > 4
""")
result9.show()
# 10. Areas with the Highest House Density (Rooms per Area)
result10 = spark.sql("""
    SELECT latitude, longitude, (total_rooms / housing_median_age) as rooms_density
    FROM california_housing
    ORDER BY rooms_density DESC
    LIMIT 10
""")
result10.show()
# Stop the Spark Session
spark.stop()




- Flight Delays Analysis using Spark SQL with PySpark********************************************


pip install pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count
# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Flights Analysis") \
    .getOrCreate()

# Load the flights dataset
flights = spark.read.format("csv") \
.option("header", "true") \
    .option("inferSchema", "true") \
    .load("/content/departuredelays.csv")
# Create a temporary view of the DataFrame
flights.createOrReplaceTempView("flights")
# Perform analysis using Spark SQL
# Example 1: Average delay by origin airport

avg_delay_by_origin = spark.sql("""
    SELECT origin, AVG(delay) as avg_delay
    FROM flights
    GROUP BY origin
    ORDER BY avg_delay DESC
    LIMIT 10
""")
avg_delay_by_origin.show()

# Example 2: Total flights and average delay by day of week

flights_by_day = spark.sql("""
    SELECT
        date,
        COUNT(*) as total_flights,
        AVG(delay) as avg_delay
    FROM flights
    GROUP BY date
    ORDER BY date
    LIMIT 7
""")
flights_by_day.collect()
flights_by_day.show()
# Example 3: Top 5 routes with the highest total delay

top_delayed_routes = spark.sql("""
    SELECT
        origin,
        destination,
        SUM(delay) as total_delay,
        COUNT(*) as flight_count
    FROM flights
    GROUP BY origin, destination
    ORDER BY total_delay DESC
    LIMIT 5
""")
top_delayed_routes.show()
# Show results
print("Top 10 origins by average delay:")
avg_delay_by_origin.show()

print("\nFlights and average delay by day (first week):")
flights_by_day.show()

print("\nTop 5 routes with highest total delay:")
top_delayed_routes.show()

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count, hour, month
# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Extended Flights Analysis") \
    .getOrCreate()

# Load the flights dataset
flights = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/content/departuredelays.csv")
# Create a temporary view of the DataFrame
flights.createOrReplaceTempView("flights")

# Example 4: Average delay by origin airport (top 10)
avg_delay_by_origin = spark.sql("""
    SELECT origin, AVG(delay) as avg_delay
    FROM flights
    GROUP BY origin
    ORDER BY avg_delay DESC
    LIMIT 10
""")
# Example 5: Top 5 busiest routes
busiest_routes = spark.sql("""
    SELECT
        origin,
        destination,
        COUNT(*) as flight_count
    FROM flights
    GROUP BY origin, destination
    ORDER BY flight_count DESC
    LIMIT 5
""")
# Example 6: Monthly flight trends

monthly_trends = spark.sql("""
    SELECT
        SUBSTRING(CAST(date AS STRING), 5, 2) as month,
        COUNT(*) as total_flights,
        AVG(delay) as avg_delay
    FROM flights
    GROUP BY SUBSTRING(CAST(date AS STRING), 5, 2)
    ORDER BY month
""")
# Example 7: Percentage of delayed flights by origin
delayed_percentage = spark.sql("""
    SELECT
        origin,
        COUNT(*) as total_flights,
        SUM(CASE WHEN delay > 0 THEN 1 ELSE 0 END) as delayed_flights,
        (SUM(CASE WHEN delay > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) as delayed_percentage
    FROM flights
    GROUP BY origin
    ORDER BY delayed_percentage DESC
    LIMIT 10
""")
# Example 8: Average delay by hour of day
delay_by_hour = spark.sql("""
    SELECT
        CAST(SUBSTRING(CAST(date AS STRING), 10, 2) AS INT) as hour,
        AVG(delay) as avg_delay
    FROM flights
    GROUP BY SUBSTRING(CAST(date AS STRING), 10, 2)
    ORDER BY hour
""")

# Show results
print("Top 10 origins by average delay:")
avg_delay_by_origin.show()

print("\nTop 5 busiest routes:")
busiest_routes.show()

print("\nMonthly flight trends:")
monthly_trends.show()

print("\nTop 10 origins by percentage of delayed flights:")
delayed_percentage.show()

print("\nAverage delay by hour of day:")
delay_by_hour.show()
# Stop the SparkSession
spark.stop()






- Extracting transforming and selecting features using Pyspark********************************************

pip install pyspark
from pyspark.sql import SparkSession
from pyspark.ml.feature import Imputer,StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler, ChiSqSelector
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
# Create a Spark Session

spark = SparkSession.builder.appName("BostonHousingFeatureEngineeringDone").getOrCreate()
# 1. Load the dataset into PySpark
#Assume we have a csv file with headers

data = spark.read.csv("/content/HousingData.csv", header = True, inferSchema = True)
#Print the schema of data types

data.printSchema()
#Define categorical and numerical columns

categorical_cols = ["CHAS", "RAD"]
numerical_cols = ["CRIM", "ZN", "INDUS", "NOX", "RM", "AGE", "DIS", "TAX", "PTRATIO", "B", "LSTAT"]
#Import DoubleType

from pyspark.sql.types import DoubleType

#2. Handling missing values and convert to appropriate types

for col_name in numerical_cols + ["MEDV"]:
  data = data.withColumn(col_name, col(col_name).cast(DoubleType()))
#Use Imputer to handle NaN values in numerical columns

imputer = Imputer(inputCols = numerical_cols, outputCols = numerical_cols)
imputer.setStrategy("mean")
#3. Handle categorical features
# for categorical columns, replace NaN with a placeholder before indexing

from pyspark.sql.functions import when,col
for cat_col in categorical_cols:
  data = data.withColumn(cat_col, when(col(cat_col).isNull(), "Unknown").otherwise(col(cat_col)))

indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_indexed", handleInvalid="keep") for col in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{col}_indexed", outputCol=f"{col}_encoded") for col in categorical_cols]

#4. Scale numerical features

assembler_num = VectorAssembler(inputCols = numerical_cols, outputCol = "num_features")
scaler = StandardScaler(inputCol = "num_features", outputCol = "scaled_num_features", withStd = True, withMean = True)

#5. combine features

encoded_cols = [f"{col}_encoded" for col in categorical_cols]
assembler = VectorAssembler(inputCols = encoded_cols + ["scaled_num_features"], outputCol = "features")

#6. Perform feature selection

median_medv = data.approxQuantile("MEDV", [0.5], 0.01)[0]
data = data.withColumn("label", (col("MEDV") > median_medv).cast("double"))

selector = ChiSqSelector(numTopFeatures = 10, featuresCol = "features", outputCol = "selected_features", labelCol = "label")

#Create and fit the pipeline

pipeline = Pipeline(stages=[imputer] + indexers + encoders + [assembler_num, scaler, assembler, selector])

try:
  model = pipeline.fit(data)
  result = model.transform(data)

  #show the result
  result.select("selected_features", "label").show(5, truncate = False)

  #Get feature importances
  feature_importances = model.stages[-1].selectedFeatures
  print("selected feature indces: ", feature_importances)

except Exception as e:
  print(f"An error occurred: {str(e)}")

#Stop the Spark session

spark.stop()







MLlib 
- Linear Regression using PySpark********************************************

!pip install pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local").appName("linea_regression_model").getOrCreate()
real_estate = spark.read.option("inferSchema", "true").csv("/content/Real estate.csv", header = True)
real_estate.printSchema()
real_estate.show(2)
real_estate.describe().show()
#VectorAssembler to transform data into feature columns

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols = [
    'X1 transaction date',
    'X2 house age',
    'X3 distance to the nearest MRT station',
    'X4 number of convenience stores',
    'X5 latitude',
    'X6 longitude'], outputCol = "features")
data_set = assembler.transform(real_estate)
data_set.select(["features","Y house price of unit area"]).show(2)
#Split into train and test data

train_data, test_data = data_set.randomSplit([0.7, 0.3])
train_data.show(truncate = False)
test_data.show(truncate = False)

# Train your model (Fit your model wth train data)

from pyspark.ml.regression import LinearRegression

lr = LinearRegression(labelCol = "Y house price of unit area")
lrModel = lr.fit(train_data)

#Perform descriptive analysis with correlation

test_stats = lrModel.evaluate(test_data)

print(f"RMSE: {test_stats.rootMeanSquaredError}")
print(f"R2: {test_stats.r2}")
print(f"MSE: {test_stats.meanSquaredError}")



- Logistic Regression using PySpark********************************************

!pip install pyspark

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ml-diabetes").getOrCreate()

df = spark.read.csv("/content/diabetes.csv", header = True, inferSchema = True)
df.printSchema()

import pandas as pd

pd.DataFrame(df.take(5), columns = df.columns).transpose()

df.show()

df.toPandas()
df.groupby("Outcome").count().toPandas()

#making numerical features

numeric_features = [t[0] for t in df.dtypes if t[1] == "int"]

numeric_features
df.select(numeric_features).describe().toPandas().transpose()

from pandas.plotting import scatter_matrix

numeric_data = df.select(numeric_features).toPandas()

axs = scatter_matrix(numeric_data, figsize = (8,8));

#Rotate axis labels and remove axis ticks

n = len(numeric_data.columns)
for i in range(n):
  v = axs[i, 0]
  v.yaxis.label.set_rotation(0)
  v.yaxis.label.set_ha("right")
  v.set_yticks(())
  h = axs[n-1, i]
  h.xaxis.label.set_rotation(90)
  h.set_xticks(())

from pyspark.sql.functions import isnull, when, count, col

df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()

#dropping unnecessary columns

dataset = df.drop("SkinThickness")
dataset = dataset.drop("Insulin")
dataset_new = dataset.drop("DiabetesPedigreeFunction")
dataset_final = dataset_new.drop("Pregnancies")

dataset_final.show()

#Assemble all the features with VectorAssembler

required_features = ["Glucose", "BloodPressure", "BMI", "Age"]

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols = required_features, outputCol = "features")

transformed_data = assembler.transform(dataset_final)
transformed_data.show()


#Split the data

(training_data, test_data) = transformed_data.randomSplit([0.8, 0.2], seed = 20)

print("Training Dataset Count: " + str(training_data.count()))
print("Test Dataset Count: " + str(test_data.count()))


from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(featuresCol = 'features', labelCol = 'Outcome', maxIter=10)
lrModel = lr.fit(training_data)
lr_predictions = lrModel.transform(test_data)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Outcome', metricName = 'accuracy')
print('Logistic Regression Accuracy:', multi_evaluator.evaluate(lr_predictions))




- Decision Tree Classification using PySpark********************************************

pip install pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('ml-diabetes').getOrCreate()

df = spark.read.csv('/diabetes.csv', header=True, inferSchema=True)
df.printSchema()

import pandas as pd

pd.DataFrame(df.take(5), columns=df.columns).transpose()
df.show()
df.toPandas()

df.groupby('Outcome').count().toPandas()
# Making numerical features
numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']
numeric_features
df.select(numeric_features).describe().toPandas().transpose()
from pandas.plotting import scatter_matrix
numeric_data = df.select(numeric_features).toPandas()

axs = scatter_matrix(numeric_data, figsize = (8,8))

# Rotate axis labels and remove axis ticks
n = len(numeric_data.columns)
for i in range(n):
    v = axs[i, 0]
    v.yaxis.label.set_rotation(0)
    v.yaxis.label.set_ha('right')
    v.set_yticks(())
    h = axs[n-1, i]
    h.xaxis.label.set_rotation(90)
    h.set_xticks(())
from pyspark.sql.functions import isnull, when, count, col

df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()
dataset = df.drop('SkinThickness')
dataset = df.drop('Insulin')
dataset.show()
dataset_new = dataset.drop('DiabetesPedigreeFunction')
dataset_new.show()
dataset_final = dataset_new.drop('Pregnancies')

dataset_final.show()
# Assemble all features with VectoeAssembler
required_features = ['Glucose','BloodPressure','BMI','Age']

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols = required_features, outputCol = 'features')

transformed_data = assembler.transform(dataset_final)
transformed_data.show()
# Split the data

(training_data, test_data) = transformed_data.randomSplit([0.8,0.2], seed =2020)

print("Training Dataset Count: " + str(training_data.count()))
print("Test Dataset Count: " + str(test_data.count()))

from pyspark.ml.classification import DecisionTreeClassifier

dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'Outcome', maxDepth = 3)

dtModel = dt.fit(training_data)

dt_predictions = dtModel.transform(test_data)

dt_predictions.select('Glucose', 'BloodPressure', 'BMI', 'Age', 'Outcome').show(10)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Outcome', metricName = 'accuracy')
print('Decision Tree Accuracy:', multi_evaluator.evaluate(dt_predictions))

Gradient-boosted Tree classifier Model

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.

from pyspark.ml.classification import GBTClassifier
gb = GBTClassifier(labelCol = 'Outcome', featuresCol = 'features')
gbModel = gb.fit(training_data)
gb_predictions = gbModel.transform(test_data)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Outcome', metricName = 'accuracy')
print('Gradient-boosted Trees Accuracy:', multi_evaluator.evaluate(gb_predictions))




- Recommendation System using PySpark********************************************
pip install pyspark
import pandas as pd
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local").appName("Movie_recommendation").config("spark.some.config.option","some-value").getOrCreate()
rating = spark.read.format("csv").option("header","true").option("inferSchema", "true").load(r"/content/ratings_small.csv")
rating = rating.drop('timestamp')

movies = spark.read.format("csv").option("header","true").option("inferSchema", "true").load(r"/content/movies_metadata.csv")
movie_data = rating.join(movies, rating.movieId == movies.id)

Columns = len(movie_data.columns)
Rows = movie_data.count()
print('Number of Columns: {}\nNumber of Rows: {}'.format(Columns, Rows))
movie_data.columns

DATA CLEANING: Replace all the zeros in the abaove mentioned fields (except “Pregnancies”) with NaN.
import numpy as np
from pyspark.sql.functions import when

movie_data = movie_data.withColumn("userId",when(movie_data.userId==0,np.nan).otherwise(movie_data.userId))
movie_data = movie_data.withColumn("id",when(movie_data.id==0,np.nan).otherwise(movie_data.id))
movie_data = movie_data.withColumn("rating",when(movie_data.rating==0,np.nan).otherwise(movie_data.rating))
movie_data = movie_data.withColumn("title",when(movie_data.title==0,np.nan).otherwise(movie_data.title))
from pyspark.sql.types import IntegerType
movie_data = movie_data.withColumn("budget", movie_data["budget"].cast(IntegerType()))

movie_data.show()

Correlations between independent variables using data visualization. Selecting different numeric and string columns, and drawing 1 plot (e.g. bar chart, histogram, boxplot, etc.) for each to summarise it

Recommendation engine

Build the recommendation model using ALS on the training data
(training,test) = movie_data.randomSplit([0.8,0.2])
# Build the recommendation model using ALS on the training data
#Fitting the Alternating Least Squares Model

from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS
from pyspark.sql.types import IntegerType # import IntegerType

# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics
als = ALS(maxIter=5,regParam=0.09,rank=25,userCol="userId",itemCol="id",ratingCol="rating",coldStartStrategy="drop",nonnegative=True)

# Convert 'id' column to IntegerType
training = training.withColumn("id", training["id"].cast(IntegerType()))

model = als.fit(training) # fit the ALS model to the training set

Generating Predictions & Model Evaluation: Evaluating a model is a core part of building an effective machine learning model. In PySpark we will be using RMSE(Root mean squared Error) as our evaluation metric. The RMSE described our error in terms of the rating column.

evaluator = RegressionEvaluator(metricName="rmse",labelCol="rating",predictionCol="prediction")

# Convert 'id' column to IntegerType in the test DataFrame
test = test.withColumn("id", test["id"].cast(IntegerType()))

predictions = model.transform(test)
rmse = evaluator.evaluate(predictions)
print("Root-mean-square error = "+str(rmse))
predictions.show()


Recommending Movies with ALS: The approach here will be simple We will be taking a single userid example 39 as features and pass it to trained ALS Model. The same way we did with the test data!

single_user = test.filter(test['userId']==39).select(['id','userId','title','genres'])
# User had 10 ratings in the test data set
# Realistically this should be some sort of hold out set!
single_user.show(truncate = False)


#Now we will use model.transform() function in order to generate recommended movies along with their predicted features.

recomendations = model.transform(single_user)
recomendations.orderBy('prediction',ascending=False).show(truncate = False )









---- Spark streaming********************************************

pip install pyspark

!pip install -q findspark
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Initialize SparkSession and SparkContext
spark = SparkSession.builder.master("local").appName("WordCount").getOrCreate()
sc = spark.sparkContext

# Initialize StreamingContext with a 10-second batch interval
ssc = StreamingContext(sc, 10)

import os

# Create a directory for the stream files
stream_dir = "/content/input.txt"
if not os.path.exists(stream_dir):
    os.mkdir(stream_dir)

# Define the text file stream
lines = ssc.textFileStream(stream_dir)

lines

# Split each line into words and count them
words = lines.flatMap(lambda line: line.split(" "))
word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

# Print the word counts to the console
word_counts.pprint()
word_counts
# Start the streaming computation
ssc.start()

import time

# Create a series of files to simulate the stream
for i in range(5):
    with open(f"/content/file{i}.txt", "w") as f:
        f.write(f"This is a streaming example file {i}.\nThis example shows streaming word count.\n")
    time.sleep(10)  # Sleep for 10 seconds to simulate real-time data

# Wait for the computation to terminate after 60 seconds
#ssc.awaitTerminationOrTimeout(60)
ssc.stop(stopSparkContext=False, stopGraceFully=True)


